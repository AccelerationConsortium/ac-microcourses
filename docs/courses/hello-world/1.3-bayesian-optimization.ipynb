{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bayesian Optimization\n",
    "\n",
    "Learn how to write a Bayesian optimization script to intelligently adjust RGB power levels to match a target color and visualize optimization results.\n",
    "\n",
    "<img src=\"../../_static/sdl-demo/grid-vs-random-vs-bayesian-ax-logo.png\" width=80%>\n",
    "\n",
    "*An optimization efficiency comparison between grid search, random search, and Bayesian optimization using Meta's Ax Platform, where Bayesian optimization is the most efficient at matching the target color.*\n",
    "\n",
    "## A Gentle Introduction to Bayesian Optimization\n",
    "\n",
    "Choosing the right optimization method depends on the specific needs and constraints of a project.\n",
    "\n",
    "✅ Watch the <a href=\"https://youtu.be/IVaWl2tL06c?si=d1uADy-Xxcj4JD_G\" target=\"_blank\">Gentle Introduction to Bayesian Optimization</a> below. This one is very important to understand the basics of Bayesian optimization and how it compares to traditional design of experiments.\n",
    "\n",
    "<div style=\"position: relative; overflow: hidden; padding-top: 50%; margin-bottom: 100px; width: 75%;\">\n",
    "    <iframe src=\"https://www.youtube.com/embed/IVaWl2tL06c?si=iSmEFYLJhXIXJam6\" title=\"YouTube video player\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: 0;\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen\" allowfullscreen=\"allowfullscreen\" mozallowfullscreen=\"mozallowfullscreen\" msallowfullscreen=\"msallowfullscreen\"  oallowfullscreen=\"oallowfullscreen\" webkitallowfullscreen=\"webkitallowfullscreen\"></iframe>\n",
    "</div>\n",
    "\n",
    "✅ Watch <a href=\"https://youtu.be/HZGCoVF3YvM?si=Gfoi-0sRvxqsTW2u\" target=\"_blank\">Bayes rule by 3Blue1Brown</a>. This will help you understand the most basic statistical concept that powers Bayesian optimization algorithms.\n",
    "\n",
    "<div style=\"position: relative; overflow: hidden; padding-top: 50%; margin-bottom: 100px; width: 75%;\">\n",
    "    <iframe src=\"https://www.youtube.com/embed/HZGCoVF3YvM?si=Gfoi-0sRvxqsTW2u\" title=\"YouTube video player\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: 0;\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen\" allowfullscreen=\"allowfullscreen\" mozallowfullscreen=\"mozallowfullscreen\" msallowfullscreen=\"msallowfullscreen\"  oallowfullscreen=\"oallowfullscreen\" webkitallowfullscreen=\"webkitallowfullscreen\"></iframe>\n",
    "</div>\n",
    "\n",
    "<!-- ✅ Review the scikit-learn documentation for [`ParameterGrid`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html) -->\n",
    "\n",
    "✅ Read the *Why use quasi-random search?* section from [a quasi-random search guide](https://developers.google.com/machine-learning/guides/deep-learning-tuning-playbook/quasi-random-search)\n",
    "\n",
    "✅ Review the scipy documentation for two types of quasi-random search: [`Sobol`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.Sobol.html) and [Latin Hypercube (LHE)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.LatinHypercube.html)\n",
    "\n",
    "✅ Spend 20 minutes exploring the [Ax Platform Docs](https://ax.dev/)\n",
    "\n",
    "✅ Complete Ax's [Service API Tutorial](https://ax.dev/tutorials/gpei_hartmann_service.html) [[Colab link](./1.3.1-ax-service-api.ipynb)]\n",
    "\n",
    "✅ Run the script below in a Python interpreter of your choice with the `ax-platform` dependency installed (i.e., `pip install ax-platform`). For convenience, you may use the Open in Colab badge in [this companion notebook](./1.3.2-ax-service-api-basic.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 01-05 18:54:05] ax.service.utils.with_db_settings_base: Ax currently requires a sqlalchemy version below 2.0. This will be addressed in a future release. Disabling SQL storage in Ax for now, if you would like to use SQL storage please install Ax with mysql extras via `pip install ax-platform[mysql]`.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the `verbose_logging` argument to `False`. Note that float values in the logs are rounded to 6 decimal points.\n",
      "[INFO 01-05 18:54:05] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x1. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 01-05 18:54:05] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x2. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 01-05 18:54:05] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[-5.0, 10.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 15.0])], parameter_constraints=[]).\n",
      "[INFO 01-05 18:54:05] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 01-05 18:54:05] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False\n",
      "[INFO 01-05 18:54:05] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5\n",
      "[INFO 01-05 18:54:05] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5\n",
      "[INFO 01-05 18:54:05] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 01-05 18:54:05] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Generated new trial 0 with parameters {'x1': 1.866556, 'x2': 12.412659}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Completed trial 0 with data: {'branin': (87.005011, None)}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Generated new trial 1 with parameters {'x1': 2.569913, 'x2': 9.499428}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Completed trial 1 with data: {'branin': (47.303482, None)}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Generated new trial 2 with parameters {'x1': 0.665525, 'x2': 5.085759}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Completed trial 2 with data: {'branin': (17.560653, None)}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Generated new trial 3 with parameters {'x1': 2.19972, 'x2': 1.36233}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Completed trial 3 with data: {'branin': (7.455261, None)}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Generated new trial 4 with parameters {'x1': 7.092661, 'x2': 7.112501}.\n",
      "[INFO 01-05 18:54:05] ax.service.ax_client: Completed trial 4 with data: {'branin': (51.458909, None)}.\n",
      "[INFO 01-05 18:54:06] ax.service.ax_client: Generated new trial 5 with parameters {'x1': -2.027617, 'x2': 0.0}.\n",
      "[INFO 01-05 18:54:06] ax.service.ax_client: Completed trial 5 with data: {'branin': (100.986227, None)}.\n",
      "[INFO 01-05 18:54:07] ax.service.ax_client: Generated new trial 6 with parameters {'x1': 3.122451, 'x2': 3.793926}.\n",
      "[INFO 01-05 18:54:08] ax.service.ax_client: Completed trial 6 with data: {'branin': (2.661517, None)}.\n",
      "[INFO 01-05 18:54:09] ax.service.ax_client: Generated new trial 7 with parameters {'x1': 4.977416, 'x2': 0.0}.\n",
      "[INFO 01-05 18:54:09] ax.service.ax_client: Completed trial 7 with data: {'branin': (14.15021, None)}.\n",
      "[INFO 01-05 18:54:11] ax.service.ax_client: Generated new trial 8 with parameters {'x1': 3.383846, 'x2': 2.136473}.\n",
      "[INFO 01-05 18:54:11] ax.service.ax_client: Completed trial 8 with data: {'branin': (0.680103, None)}.\n",
      "[INFO 01-05 18:54:13] ax.service.ax_client: Generated new trial 9 with parameters {'x1': 10.0, 'x2': 0.0}.\n",
      "[INFO 01-05 18:54:13] ax.service.ax_client: Completed trial 9 with data: {'branin': (10.960889, None)}.\n",
      "[WARNING 01-05 18:54:13] ax.modelbridge.cross_validation: Metric branin was unable to be reliably fit.\n",
      "[WARNING 01-05 18:54:13] ax.service.utils.best_point: Model fit is poor; falling back on raw data for best point.\n",
      "[WARNING 01-05 18:54:13] ax.service.utils.best_point: Model fit is poor and data on objective metric branin is noisy; interpret best points results carefully.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "\n",
    "obj1_name = \"branin\"\n",
    "\n",
    "def branin(x1, x2):\n",
    "    y = float(\n",
    "        (x2 - 5.1 / (4 * math.pi**2) * x1**2 + 5.0 / math.pi * x1 - 6.0) ** 2\n",
    "        + 10 * (1 - 1.0 / (8 * math.pi)) * math.cos(x1)\n",
    "        + 10\n",
    "    )\n",
    "\n",
    "    return y\n",
    "\n",
    "ax_client = AxClient()\n",
    "ax_client.create_experiment(\n",
    "    parameters=[\n",
    "        {\"name\": \"x1\", \"type\": \"range\", \"bounds\": [-5.0, 10.0]},\n",
    "        {\"name\": \"x2\", \"type\": \"range\", \"bounds\": [0.0, 15.0]},\n",
    "    ],\n",
    "    objectives={\n",
    "        obj1_name: ObjectiveProperties(minimize=True),\n",
    "    },\n",
    ")\n",
    "\n",
    "for _ in range(10):\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    results = branin(\n",
    "        parameters[\"x1\"],\n",
    "        parameters[\"x2\"],\n",
    "    )\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=results)\n",
    "\n",
    "best_parameters, metrics = ax_client.get_best_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "For the mathematically inclined or those who would like to understand the theory behind Bayesian optimization, you are encouraged to work through [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/), which will give you a better sense of what Gaussian processes are and how Bayesian inference is used to perform Gaussian process regression. Note that Gaussian processes are the most widely used surrogate model in Bayesian optimization applications. Next, you can work through [Exploring Bayesian Optimization](https://distill.pub/2020/bayesian-optimization/). These two resources will help you to develop intuition about Bayesian optimization by understanding the underlying theory.\n",
    "\n",
    "You may also be interested in the following resources:\n",
    "\n",
    "- [How are materials discovered?](https://youtu.be/RRNcqJSJ6vc?si=_sdNLUZFbW-3APrJ) YouTube video\n",
    "- [Grid vs. random vs. Bayesian optimization](https://towardsdatascience.com/grid-search-vs-random-search-vs-bayesian-optimization-2e68f57c3c46) general machine learning tutorial (requires a Medium account)\n",
    "- [Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html) scikit-learn tutorial\n",
    "- [The Olympus benchmarking package](https://aspuru-guzik-group.github.io/olympus/classes/planners/index.html) with a variety of other optimization algorithms documented\n",
    "- [A `self-driving-lab-demo` discussion](https://github.com/sparks-baird/self-driving-lab-demo/discussions/57) which has a list of optimization algorithms \n",
    "- [Honegumi](https://github.com/sgbaird/honegumi), an interactive \"skeleton code\" generator for optimization packages\n",
    "\n",
    "## \n",
    "\n",
    "That's it! You've completed the tutorial 🎉. Return to the course website to do a knowledge check."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac-microcourses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
